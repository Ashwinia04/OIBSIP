# -*- coding: utf-8 -*-
"""Task1_IRIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CFAS53yMwVrvaMy-w71U1hWfFI_W71Mf
"""

from sklearn import datasets
import pandas as pd
import numpy as np

iris=datasets.load_iris()

iris

iris.keys()

species=[]
for i in range(len(iris['target'])):
    if iris['target'][i]==0:
        species.append('Setosa')
    elif iris['target'][i]==1:
        species.append('Versicolor')
    else:
        species.append('Virginica')

iris['species']=species

iris

#to check 3targets are equal
iris.groupby('species').size()

#statistical analysis
iris.describe()

#missing values
iris.isnull().sum()

#train and test split
from sklearn.model_selection import train_test_split
X=iris.drop(['target','species'],axis=1)
y=iris['target']

X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=50)

X_train.shape

Y_train.shape

from sklearn.linear_model import LogisticRegression
log_model=LogisticRegression()

log_model.fit(X_train,Y_train)

pred=log_model.predict(X_test)

training_prediction=log_model.predict(X_train)

testing_prediction=log_model.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

#performance analysis for training data
print(classification_report(Y_train,training_prediction))
#performance analysis for test data
print(classification_report(Y_test,testing_prediction))

#KNN
data_iris=iris

data_iris

X=data_iris.drop(['target','species'],axis=1)
y=data_iris['target']

from sklearn.preprocessing import StandardScaler
Scaler=StandardScaler()

Scaler.fit(X)

scaled_features=Scaler.transform(X)

scaled_features

data_iris.columns[:-2]

df_feat=pd.DataFrame(scaled_features,columns=data_iris.columns[:-2])

df_feat

X=df_feat

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,random_state=10)

from sklearn.neighbors import KNeighborsClassifier

knn_model=KNeighborsClassifier(n_neighbors=1)
knn_model.fit(X_train,Y_train)
pred=knn_model.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(Y_test,pred))
print(confusion_matrix(Y_test,pred))

error_rate=[]
for i in range(1,40):
    knn=KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,Y_train)
    pred_i=knn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue',linestyle='dashed',marker='o',markerfacecolor='red',markersize=10)
plt.title('Error rate vs K-value')
plt.xlabel('K')
plt.ylabel('ErrorÂ Rate')

knn_model=KNeighborsClassifier(n_neighbors=17)
knn_model.fit(X_train,Y_train)
pred=knn_model.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(Y_test,pred))
print(confusion_matrix(Y_test,pred))

#Decision tree
from sklearn.tree import DecisionTreeClassifier
clf=DecisionTreeClassifier()
clf=clf.fit(X_train,Y_train)
pred_tree=clf.predict(X_test)

print(classification_report(Y_test,pred_tree))
print(confusion_matrix(Y_test,pred_tree))

print(classification_report(Y_test,pred_tree))
print(confusion_matrix(Y_test,pred_tree))

from sklearn import tree
tree.plot_tree(clf,feature_names=X_train.columns)

